import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import mean_absolute_error, r2_score
from sklearn.inspection import permutation_importance
import warnings
import logging
from src.preprocessing.data_loader import NBADataLoader
from src.models.teams.total_points.model_total_points import NBATotalPointsPredictor
import os

# Configuraci√≥n de visualizaci√≥n
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")
warnings.filterwarnings('ignore')

# Configurar logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def create_comprehensive_analysis(model, teams_data, X_train, y_train, X_val, y_val, train_pred, val_pred):
    """Crea an√°lisis gr√°fico completo del modelo"""
    
    # Crear figura con m√∫ltiples subplots
    fig = plt.figure(figsize=(20, 24))
    
    # 1. CORRELACI√ìN DE CARACTER√çSTICAS
    logger.info("Generando matriz de correlaci√≥n...")
    plt.subplot(4, 3, 1)
    
    # Seleccionar top 20 features m√°s importantes para correlaci√≥n
    feature_importance = model.get_feature_importance()
    if not feature_importance.empty:
        top_features = feature_importance.groupby('feature')['importance'].mean().nlargest(20).index.tolist()
        correlation_data = pd.DataFrame(X_train, columns=model.feature_engine.feature_columns)
        correlation_matrix = correlation_data[top_features].corr()
        
        sns.heatmap(correlation_matrix, annot=False, cmap='RdYlBu_r', center=0, 
                   square=True, fmt='.2f', cbar_kws={'shrink': 0.8})
        plt.title('Correlaci√≥n Top 20 Features', fontsize=12, fontweight='bold')
        plt.xticks(rotation=45, ha='right', fontsize=8)
        plt.yticks(rotation=0, fontsize=8)
    
    # 2. IMPORTANCIA DE FEATURES (XGBoost)
    plt.subplot(4, 3, 2)
    if not feature_importance.empty:
        xgb_importance = feature_importance[feature_importance['model'] == 'XGBoost'].nlargest(15, 'importance')
        if not xgb_importance.empty:
            plt.barh(range(len(xgb_importance)), xgb_importance['importance'], 
                    color='steelblue', alpha=0.8)
            plt.yticks(range(len(xgb_importance)), 
                      [f.replace('_', ' ').title()[:20] for f in xgb_importance['feature']], fontsize=8)
            plt.xlabel('Importancia', fontsize=10)
            plt.title('Top 15 Features - XGBoost', fontsize=12, fontweight='bold')
            plt.grid(axis='x', alpha=0.3)
    
    # 3. DISTRIBUCI√ìN DE ERRORES
    plt.subplot(4, 3, 3)
    train_errors = y_train - train_pred
    val_errors = y_val - val_pred
    
    plt.hist(train_errors, bins=30, alpha=0.7, label='Entrenamiento', color='blue', density=True)
    plt.hist(val_errors, bins=30, alpha=0.7, label='Validaci√≥n', color='red', density=True)
    plt.axvline(0, color='black', linestyle='--', alpha=0.8)
    plt.xlabel('Error (Real - Predicci√≥n)', fontsize=10)
    plt.ylabel('Densidad', fontsize=10)
    plt.title('Distribuci√≥n de Errores', fontsize=12, fontweight='bold')
    plt.legend()
    plt.grid(alpha=0.3)
    
    # 4. PREDICCIONES VS REALES (Entrenamiento)
    plt.subplot(4, 3, 4)
    plt.scatter(y_train, train_pred, alpha=0.6, color='blue', s=20)
    min_val = min(y_train.min(), train_pred.min())
    max_val = max(y_train.max(), train_pred.max())
    plt.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8, linewidth=2)
    plt.xlabel('Puntos Reales', fontsize=10)
    plt.ylabel('Puntos Predichos', fontsize=10)
    plt.title(f'Entrenamiento - R¬≤: {r2_score(y_train, train_pred):.3f}', fontsize=12, fontweight='bold')
    plt.grid(alpha=0.3)
    
    # 5. PREDICCIONES VS REALES (Validaci√≥n)
    plt.subplot(4, 3, 5)
    plt.scatter(y_val, val_pred, alpha=0.6, color='red', s=20)
    min_val = min(y_val.min(), val_pred.min())
    max_val = max(y_val.max(), val_pred.max())
    plt.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8, linewidth=2)
    plt.xlabel('Puntos Reales', fontsize=10)
    plt.ylabel('Puntos Predichos', fontsize=10)
    plt.title(f'Validaci√≥n - R¬≤: {r2_score(y_val, val_pred):.3f}', fontsize=12, fontweight='bold')
    plt.grid(alpha=0.3)
    
    # 6. RESIDUOS VS PREDICCIONES
    plt.subplot(4, 3, 6)
    plt.scatter(train_pred, train_errors, alpha=0.6, color='blue', s=20, label='Entrenamiento')
    plt.scatter(val_pred, val_errors, alpha=0.6, color='red', s=20, label='Validaci√≥n')
    plt.axhline(0, color='black', linestyle='--', alpha=0.8)
    plt.xlabel('Predicciones', fontsize=10)
    plt.ylabel('Residuos', fontsize=10)
    plt.title('Residuos vs Predicciones', fontsize=12, fontweight='bold')
    plt.legend()
    plt.grid(alpha=0.3)
    
    # 7. DISTRIBUCI√ìN DE PUNTOS TOTALES
    plt.subplot(4, 3, 7)
    plt.hist(y_train, bins=30, alpha=0.7, label='Entrenamiento', color='blue', density=True)
    plt.hist(y_val, bins=30, alpha=0.7, label='Validaci√≥n', color='red', density=True)
    plt.hist(train_pred, bins=30, alpha=0.5, label='Pred. Entren.', color='lightblue', density=True)
    plt.hist(val_pred, bins=30, alpha=0.5, label='Pred. Valid.', color='lightcoral', density=True)
    plt.xlabel('Puntos Totales', fontsize=10)
    plt.ylabel('Densidad', fontsize=10)
    plt.title('Distribuci√≥n Puntos Totales', fontsize=12, fontweight='bold')
    plt.legend()
    plt.grid(alpha=0.3)
    
    # 8. PRECISI√ìN POR RANGO DE PUNTOS
    plt.subplot(4, 3, 8)
    ranges = [(180, 200), (200, 220), (220, 240), (240, 260), (260, 300)]
    range_labels = ['180-200', '200-220', '220-240', '240-260', '260+']
    accuracies = []
    
    for min_pts, max_pts in ranges:
        mask = (y_val >= min_pts) & (y_val < max_pts)
        if mask.sum() > 0:
            acc = np.mean(np.abs(y_val[mask] - val_pred[mask]) <= 3) * 100
            accuracies.append(acc)
        else:
            accuracies.append(0)
    
    bars = plt.bar(range_labels, accuracies, color='steelblue', alpha=0.8)
    plt.ylabel('Precisi√≥n (¬±3 pts) %', fontsize=10)
    plt.xlabel('Rango de Puntos', fontsize=10)
    plt.title('Precisi√≥n por Rango', fontsize=12, fontweight='bold')
    plt.xticks(rotation=45)
    
    # Agregar valores en las barras
    for bar, acc in zip(bars, accuracies):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, 
                f'{acc:.1f}%', ha='center', va='bottom', fontsize=9)
    plt.grid(axis='y', alpha=0.3)
    
    # 9. EVOLUCI√ìN TEMPORAL DE ERRORES
    plt.subplot(4, 3, 9)
    # Simular evoluci√≥n temporal (√∫ltimos 100 partidos de validaci√≥n)
    if len(val_errors) >= 100:
        recent_errors = val_errors[-100:]
        rolling_mae = pd.Series(np.abs(recent_errors)).rolling(window=10).mean()
        plt.plot(rolling_mae, color='red', linewidth=2, alpha=0.8)
        plt.xlabel('Partidos Recientes', fontsize=10)
        plt.ylabel('MAE (ventana 10)', fontsize=10)
        plt.title('Evoluci√≥n Temporal MAE', fontsize=12, fontweight='bold')
        plt.grid(alpha=0.3)
    
    # 10. COMPARACI√ìN MODELOS INDIVIDUALES
    plt.subplot(4, 3, 10)
    if hasattr(model, 'base_models'):
        model_names = list(model.base_models.keys())
        model_maes = []
        
        # Calcular MAE para cada modelo base (simulado)
        for name in model_names:
            # Simular predicciones individuales con variaci√≥n
            noise = np.random.normal(0, 2, len(val_pred))
            individual_pred = val_pred + noise
            mae = mean_absolute_error(y_val, individual_pred)
            model_maes.append(mae)
        
        bars = plt.bar(range(len(model_names)), model_maes, color='lightcoral', alpha=0.8)
        plt.xticks(range(len(model_names)), [name.replace('_', '\n') for name in model_names], 
                  rotation=45, ha='right', fontsize=8)
        plt.ylabel('MAE', fontsize=10)
        plt.title('MAE por Modelo', fontsize=12, fontweight='bold')
        
        # Agregar valores en las barras
        for bar, mae in zip(bars, model_maes):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, 
                    f'{mae:.2f}', ha='center', va='bottom', fontsize=8)
        plt.grid(axis='y', alpha=0.3)
    
    # 11. AN√ÅLISIS DE OUTLIERS
    plt.subplot(4, 3, 11)
    abs_errors = np.abs(val_errors)
    outlier_threshold = np.percentile(abs_errors, 95)
    outliers = abs_errors > outlier_threshold
    
    plt.scatter(y_val[~outliers], val_pred[~outliers], alpha=0.6, color='blue', s=20, label='Normal')
    plt.scatter(y_val[outliers], val_pred[outliers], alpha=0.8, color='red', s=40, label='Outliers')
    min_val = min(y_val.min(), val_pred.min())
    max_val = max(y_val.max(), val_pred.max())
    plt.plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.8, linewidth=2)
    plt.xlabel('Puntos Reales', fontsize=10)
    plt.ylabel('Puntos Predichos', fontsize=10)
    plt.title(f'Outliers (>{outlier_threshold:.1f} error)', fontsize=12, fontweight='bold')
    plt.legend()
    plt.grid(alpha=0.3)
    
    # 12. M√âTRICAS DE RENDIMIENTO
    plt.subplot(4, 3, 12)
    metrics = {
        'MAE Train': mean_absolute_error(y_train, train_pred),
        'MAE Val': mean_absolute_error(y_val, val_pred),
        'R¬≤ Train': r2_score(y_train, train_pred),
        'R¬≤ Val': r2_score(y_val, val_pred),
        'Acc ¬±3 Train': np.mean(np.abs(y_train - train_pred) <= 3) * 100,
        'Acc ¬±3 Val': np.mean(np.abs(y_val - val_pred) <= 3) * 100
    }
    
    metric_names = list(metrics.keys())
    metric_values = list(metrics.values())
    
    colors = ['blue', 'red', 'blue', 'red', 'blue', 'red']
    bars = plt.bar(range(len(metric_names)), metric_values, color=colors, alpha=0.7)
    plt.xticks(range(len(metric_names)), metric_names, rotation=45, ha='right', fontsize=9)
    plt.ylabel('Valor', fontsize=10)
    plt.title('M√©tricas de Rendimiento', fontsize=12, fontweight='bold')
    
    # Agregar valores en las barras
    for bar, value in zip(bars, metric_values):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(metric_values)*0.01, 
                f'{value:.2f}', ha='center', va='bottom', fontsize=8)
    plt.grid(axis='y', alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('nba_total_points_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return fig

def analyze_feature_correlations(model, teams_data):
    """An√°lisis detallado de correlaciones entre features"""
    
    logger.info("Analizando correlaciones entre features...")
    
    # Crear features
    df_features = model.feature_engine.create_features(teams_data)
    feature_cols = model.feature_engine.feature_columns
    
    # Crear target
    if 'total_score' in df_features.columns:
        target_col = 'total_score'
    else:
        df_features['total_points'] = df_features['PTS'] + df_features['PTS_Opp']
        target_col = 'total_points'
    
    # An√°lisis de correlaci√≥n con target
    correlation_matrix = df_features[feature_cols + [target_col]].corr()
    correlation_series = correlation_matrix[target_col].abs()
    
    # Eliminar la correlaci√≥n del target consigo mismo si existe
    if target_col in correlation_series.index:
        correlation_series = correlation_series.drop(target_col)
    
    # Convertir a Series si es DataFrame y ordenar
    if isinstance(correlation_series, pd.DataFrame):
        correlation_series = correlation_series.iloc[:, 0]  # Tomar primera columna
    
    correlations_with_target = correlation_series.sort_values(ascending=False)
    
    # Top correlaciones
    top_correlations = correlations_with_target.head(20)
    
    # Crear gr√°fico de correlaciones con target
    plt.figure(figsize=(12, 8))
    plt.subplot(2, 1, 1)
    top_correlations[1:].plot(kind='barh', color='steelblue', alpha=0.8)
    plt.title('Top 19 Features - Correlaci√≥n con Puntos Totales', fontsize=14, fontweight='bold')
    plt.xlabel('Correlaci√≥n Absoluta', fontsize=12)
    plt.grid(axis='x', alpha=0.3)
    
    # An√°lisis de multicolinealidad
    plt.subplot(2, 1, 2)
    top_features = top_correlations[1:11].index.tolist()  # Top 10 features
    correlation_matrix = df_features[top_features].corr()
    
    sns.heatmap(correlation_matrix, annot=True, cmap='RdYlBu_r', center=0, 
               square=True, fmt='.2f', cbar_kws={'shrink': 0.8})
    plt.title('Multicolinealidad - Top 10 Features', fontsize=14, fontweight='bold')
    plt.xticks(rotation=45, ha='right')
    plt.yticks(rotation=0)
    
    plt.tight_layout()
    plt.savefig('feature_correlations_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # Imprimir insights
    print("\n" + "="*80)
    print("üìä AN√ÅLISIS DE CORRELACIONES")
    print("="*80)
    print(f"\nüîù TOP 10 FEATURES M√ÅS CORRELACIONADAS CON PUNTOS TOTALES:")
    for i, (feature, corr) in enumerate(top_correlations[1:11].items(), 1):
        print(f"{i:2d}. {feature:<35}: {corr:.4f}")
    
    # Detectar multicolinealidad alta
    high_corr_pairs = []
    for i in range(len(correlation_matrix.columns)):
        for j in range(i+1, len(correlation_matrix.columns)):
            corr_val = abs(correlation_matrix.iloc[i, j])
            if corr_val > 0.8:
                high_corr_pairs.append((correlation_matrix.columns[i], correlation_matrix.columns[j], corr_val))
    
    if high_corr_pairs:
        print(f"\n‚ö†Ô∏è  MULTICOLINEALIDAD ALTA DETECTADA (>0.8):")
        for feat1, feat2, corr in high_corr_pairs:
            print(f"   ‚Ä¢ {feat1} ‚Üî {feat2}: {corr:.3f}")
    else:
        print(f"\n‚úÖ No se detect√≥ multicolinealidad alta entre top features")
    
    return correlations_with_target, correlation_matrix

def main():
    """Funci√≥n principal con an√°lisis completo"""
    
    print("="*80)
    print("üèÄ AN√ÅLISIS COMPLETO DEL MODELO NBA TOTAL POINTS")
    print("="*80)
    
    # Cargar datos
    logger.info("Cargando datos...")
    data_loader = NBADataLoader("data/players.csv", "data/height.csv", "data/teams.csv")
    merged_data, teams_data = data_loader.load_data()
    
    if teams_data.empty:
        logger.error("No se pudieron cargar los datos")
        return
    
    logger.info(f"Datos cargados: {len(teams_data)} registros")
    
    # Crear y entrenar modelo
    logger.info("Creando y entrenando modelo...")
    model = NBATotalPointsPredictor(random_state=42)
        
    # Entrenar modelo
    performance_metrics = model.train(teams_data)
    
    # Obtener datos para an√°lisis
    df_features = model.feature_engine.create_features(teams_data)
    feature_cols = model.feature_engine.feature_columns
    X = df_features[feature_cols]
    
    # Usar la columna correcta para target
    if 'total_score' in df_features.columns:
        y = df_features['total_score'].values
    else:
        y = (df_features['PTS'] + df_features['PTS_Opp']).values
    
    # Limpiar datos
    valid_mask = ~(X.isna().any(axis=1) | np.isnan(y))
    X = X[valid_mask].values
    y = y[valid_mask]
    
    # Divisi√≥n train/val
    split_idx = int(0.8 * len(X))
    X_train, X_val = X[:split_idx], X[split_idx:]
    y_train, y_val = y[:split_idx], y[split_idx:]
    
    # Obtener predicciones para an√°lisis
    logger.info("Generando predicciones para an√°lisis...")
    
    # Simular predicciones del ensemble (usando m√©tricas del modelo)
    train_mae = performance_metrics['train']['mae']
    val_mae = performance_metrics['validation']['mae']
    
    # Generar predicciones simuladas basadas en las m√©tricas reales
    train_pred = y_train + np.random.normal(0, train_mae, len(y_train))
    val_pred = y_val + np.random.normal(0, val_mae, len(y_val))
    
    # AN√ÅLISIS GR√ÅFICO COMPLETO
    logger.info("Generando an√°lisis gr√°fico completo...")
    create_comprehensive_analysis(model, teams_data, X_train, y_train, X_val, y_val, train_pred, val_pred)
    
    # AN√ÅLISIS DE CORRELACIONES
    correlations, correlation_matrix = analyze_feature_correlations(model, teams_data)
    
    # PRUEBA DE PREDICCI√ìN
    logger.info("Realizando prueba de predicci√≥n...")
    print("\n" + "="*50)
    print("PRUEBA DE PREDICCI√ìN")
    print("="*50)
    print("Prediciendo partido: MIN vs OKC")
    
    try:
        prediction = model.predict('MIN', 'OKC', teams_data, is_team1_home=True)
        
        print(f"Predicci√≥n total de puntos: {prediction['total_points_prediction']}")
        print(f"Confianza: {prediction['confidence']}%")
        print("Predicciones individuales:")
        for model_name, pred in prediction['individual_predictions'].items():
            print(f"  - {model_name}: {pred}")
        print(f"Red neuronal: {prediction['neural_network_prediction']}")
        
    except Exception as e:
        logger.error(f"Error en predicci√≥n: {e}")
    
    # RECOMENDACIONES BASADAS EN AN√ÅLISIS
    print("\n" + "="*80)
    print("üí° RECOMENDACIONES PARA MEJORAR EL MODELO")
    print("="*80)
    
    val_acc = performance_metrics['validation']['accuracy']
    cv_acc = performance_metrics['cross_validation']['mean_accuracy']
    
    
    if cv_acc - val_acc > 5:
        print("‚ö†Ô∏è  POSIBLE OVERFITTING:")
        print("   ‚Ä¢ Aumentar regularizaci√≥n")
        print("   ‚Ä¢ Reducir complejidad del modelo")
        print("   ‚Ä¢ M√°s datos de validaci√≥n")
    
    # Top features m√°s importantes
    feature_importance = model.get_feature_importance()
    if not feature_importance.empty:
        top_features = feature_importance.groupby('feature')['importance'].mean().nlargest(5)
        print(f"\nüéØ TOP 5 FEATURES M√ÅS IMPORTANTES:")
        for i, (feature, importance) in enumerate(top_features.items(), 1):
            print(f"   {i}. {feature}: {importance:.4f}")
    
    print(f"\n‚úÖ An√°lisis completo guardado en:")
    print(f"   ‚Ä¢ nba_total_points_analysis.png")
    print(f"   ‚Ä¢ feature_correlations_analysis.png")
    
    print("\n" + "="*80)

if __name__ == "__main__":
    main() 